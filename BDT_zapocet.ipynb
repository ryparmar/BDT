{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Na vypracování testu máte 90 minut. Očekávanými a hodnocenými výstupy jsou:\n",
    "1.\tTextový soubor s názvem váš_login.txt (např. pascepet.txt) obsahující zdrojový kód příkazů, kterými jste provedli zadání. Pokud máte zároveň něco zjistit nebo vypsat, zaznamenejte to přímo do tohoto souboru, ideálně do komentáře. Tento soubor na konci zašlete mailem na adresu **jan.hucin@profinit.eu** a podepište se svým celým jménem.\n",
    "2.\tExistence a vlastnosti souborů, adresářů, tabulek zkopírovaných či vytvořených při plnění zadání. Tyto soubory apod. neposílejte, zhodnotíme je přímo na clusteru.\n",
    "\n",
    "V jednotlivých oblastech testu se hodnotí každý úkol nebo jeho část. Pokud si s nějakou částí zadání nebudete vědět rady, můžete ji přeskočit nebo zadání splnit bez této části, počet bodů se pak přiměřeně sníží.\n",
    "\n",
    "### HDFS operace (3 body)\n",
    "- Do svého uživatelského adresáře na HDFS (`/user/váš_login`) zkopírujte z lokálního filesystemu na metacentru z podadresáře `/home/pascepet/fel_bigdata/test` soubor `iris.csv`.\n",
    "- Ve svém uživatelském adresáři na HDFS (`/user/váš_login`) založte podadresář `final_test`.\n",
    "- Do tohoto podadresáře zkopírujte z HDFS z podadresáře `/user/pascepet/final_test` soubor `data-trans.csv`.\n",
    "- Nastavte na svém uživatelském adresáři na HDFS pro veřejnost právo read a execute.\n",
    "\n",
    "### Práce s Hive (9 bodů)\n",
    "Data ze souboru `data-trans.csv`, která jste si zkopírovali do svého uživatelského adresáře na HDFS, importujte vhodným postupem jako tabulku do Hive, a to do vaší databáze (nemáte-li ji ještě založenou, založte ji se stejným názvem, jako je váš login). Požadované vlastnosti výsledné tabulky:\n",
    "- Tabulka bude interní (managed) a název tabulky bude data_trans.\n",
    "- Pole a jejich typy v tabulce budou: id_from (celé číslo), id_to (celé číslo), datum (řetězec), amt (reálné číslo). U tabulky bude zadán partitioning podle měsíce (podřetězec z pole datum).\n",
    "- Formát tabulky bude ORC s kompresí ZLIB.\n",
    "- V tabulce budou jen úplné řádky (v žádném poli nesmí být hodnota null), neúplné řádky v původních datech ignorujte.\n",
    "Zjistěte, kolik záznamů má datum v 6. měsíci (červen).\n",
    "\n",
    "### Spark RDD (9 bodů)\n",
    "Soubor `data-posloupnosti.txt` v adresáři `/user/pascepet/final_test` obsahuje řádky ve struktuře nazev souboru, řetězec 0/1. V každém řádku nás bude zajímat podíl jedniček v řetězci (počet jedniček dělený délkou řetězce) a jeho délka.\n",
    "- Vypište názvy tří souborů s největším podílem jedniček v řetězci.\n",
    "- Zjistěte počet souborů, u nichž je podíl jedniček menší než 0,25.\n",
    "- Jaká je maximální délka řetězce 0/1 v datech?\n",
    "\n",
    "### Spark SQL (9 bodů)\n",
    "Soubor `train_users_2.csv` na HDFS v adresáři `/user/pascepet/final_test/airbnb` obsahuje strukturovaná data z registrací v systému Airbnb. Soubor obsahuje hlavičku s názvy polí, oddělovač je čárka (,). Všechny úlohy budete provádět jen nad těmi řádky, kde v poli country_destination není hodnota \"NDF\".\n",
    "- Zkontrolujte, že každé id je v datech uvedeno jen jednou.\n",
    "- Který prohlížeč (first_browser) má druhou nejvyšší četnost?\n",
    "- Jaké je minimální a maximální datum registrace uživatele (date_account_created) ze země Itálie (IT)?\n",
    "- Kolik žen (gender) se registrovalo přes Facebook (signup_method)?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
